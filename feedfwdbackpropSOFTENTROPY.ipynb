{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sigmoid', 'softmax']\n",
      "training ...\n",
      "x1\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "softmax\n",
      "backPROP\n",
      "testing ...\n",
      "softmax\n",
      "soft\n",
      "[[ 0.70109693]\n",
      " [ 0.29890307]]\n",
      "cross\n",
      "0.355109125206\n",
      "test results\n",
      "[0.35510912520646509]\n",
      "total regression\n",
      "[0, 0.35510912520646509]\n",
      "regression cost: 0.355109\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "layers=[4,3,2] #specify the layer sizes here \n",
    "\n",
    "\n",
    "#bala: added options for having different activations for different layers\n",
    "global activation_codes \n",
    "activation_codes= []  \n",
    "for ind in range(len(layers)-1):\n",
    "    #print('ind: %d' %(ind))\n",
    "    #activation_codes.append(\"linear\") \n",
    "    #activation_codes.append(\"sigmoid\") \n",
    "    #you can modify the activation function here depending upon your needs\n",
    "    if ind==0:\n",
    "        activation_codes.append(\"sigmoid\") #first and second are sigmoid \n",
    "    else:\n",
    "        #activation_codes.append(\"linear\")\n",
    "        activation_codes.append(\"softmax\")\n",
    "        \n",
    "    #if ind==len(layers)-2:\n",
    "    #    activation_codes.append(\"linear\") #the last layer has linear activation \n",
    "    #else:\n",
    "    #    activation_codes.append(\"sigmoid\") #all other layers have sigmoid activation\n",
    "\n",
    "print(activation_codes)\n",
    "\n",
    "\n",
    "global cost_code \n",
    "#cost_code = \"mse\" #you can use cross_entropy, hinge, etc. \n",
    "cost_code=\"cross\"\n",
    "global num_layers \n",
    "num_layers=len(layers)\n",
    "\n",
    "\n",
    "eta=0.001 #learning rate\n",
    "epochs=30 #num epochs\n",
    "mini_batch_size=1 #currently mini batch size is just one\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    sigm=sigmoid(z)\n",
    "    return sigm*(1-sigm)\n",
    "\n",
    "def linear_prime(z):\n",
    "    return (z-z+1)\n",
    "\n",
    "def relu_prime(z):\n",
    "    zz = [1 if ele>0 else 0 for ele in z]\n",
    "    return (zz.transpose)\n",
    "\n",
    "def soft_max(z):\n",
    "    p= np.exp(z) / sum(np.exp(z))\n",
    "    return(p)\n",
    "\n",
    "def activation_prime(z,activation_fn):\n",
    "    if activation_fn=='sigmoid':\n",
    "        deriv = sigmoid_prime(z)\n",
    "    elif activation_fn=='linear':\n",
    "        deriv= linear_prime(z)\n",
    "    elif activation_fn=='relu':\n",
    "        deriv= relu_prime(z)\n",
    "    #elif activation_fn=='tanh': #need to add this later\n",
    "        #x=tanh(0,np.dot(w,x)+b)\n",
    "    else:\n",
    "        print('Unknown activation function:%s Please check !' %(activation_fn))\n",
    "        deriv = soft_max(z)\n",
    "    return deriv\n",
    "\n",
    "def cost_derivative(output_activation,y,cost_code):\n",
    "    y=np.reshape(y,(-1,1))\n",
    "    if cost_code=='mse':\n",
    "        deriv = (output_activation-y)\n",
    "    elif cost_code=='cross':\n",
    "        #print('cost code not yet implemented. using default cost mse')\n",
    "        deriv = (output_activation-y)\n",
    "    return deriv \n",
    "\n",
    "def feed_forward_compute_activation(b,w,x,activation_fn):\n",
    "    x=np.dot(w,x)+b\n",
    "    if activation_fn=='sigmoid':\n",
    "        x_act = sigmoid(x)\n",
    "    elif activation_fn=='linear':\n",
    "        x_act=x\n",
    "    elif activation_fn=='relu':\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        x_act=np.max(0,x)\n",
    "    #elif activation_fn=='tanh': #need to add this later\n",
    "        #x=tanh(0,np.dot(w,x)+b)\n",
    "    else:\n",
    "        #print('Unknown activation function:%s Please check !' %(activation_fn))\n",
    "        print(\"softmax\")\n",
    "        x_act=soft_max(x) #call sigmoid by default \n",
    "    return x, x_act\n",
    "\n",
    "def feed_forward(biases,weights,x):\n",
    "    activation = x\n",
    "    activations = [x]\n",
    "    zs = []\n",
    "    layer = 0\n",
    "    for b, w in zip(biases,weights):\n",
    "        z, activation = feed_forward_compute_activation(b,w,activation,activation_codes[layer])\n",
    "        #z = np.dot(w,activation)+b\n",
    "        zs.append(z)\n",
    "        #activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        layer = layer+1\n",
    "    return zs, activations \n",
    "\n",
    "def feed_forward_eval(biases,weights,x):\n",
    "    activation = x\n",
    "    layer = 0\n",
    "    for b, w in zip(biases,weights):\n",
    "        z, activation = feed_forward_compute_activation(b,w,activation,activation_codes[layer])\n",
    "        layer = layer + 1\n",
    "    return activation\n",
    "\n",
    "def backprop(biases,weights,x,y,zs,activations):\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    #separate handling of last layer\n",
    "    layer = -1\n",
    "    #print(activations[layer],y)\n",
    "    #delta = cost_derivative(activations[layer],y,cost_code)*activation_prime(zs[layer], activation_codes[layer]) # sigmoid_prime()\n",
    "    delta = cost_derivative(activations[layer],y,cost_code)\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    for l in range(2, num_layers):\n",
    "        sp = activation_prime(zs[-l], activation_codes[-l])\n",
    "        delta = np.dot(weights[-l+1].transpose(),delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta,activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)\n",
    "\n",
    "\n",
    "def update_mini_batch_sgd(biases,weights,mini_batch,eta):\n",
    "    nabla_b=[np.zeros(b.shape) for b in biases]\n",
    "    nabla_w=[np.zeros(w.shape) for w in weights]\n",
    "    for x,y in mini_batch:\n",
    "        print(\"x1\")\n",
    "        print(x)\n",
    "        zs, activations = feed_forward(biases,weights,x)\n",
    "        print(\"backPROP\")\n",
    "        delta_nabla_b,delta_nabla_w=backprop(biases,weights,x,y,zs, activations)\n",
    "        nabla_b=[nb+dnb for nb,dnb in zip(nabla_b,delta_nabla_b)]\n",
    "        nabla_w=[nw+dnw for nw,dnw in zip(nabla_w,delta_nabla_w)]\n",
    "        \n",
    "    weights=[w-(eta/(len(mini_batch)))*nw for w,nw in zip(weights,nabla_w)]\n",
    "    biases=[b-(eta/len(mini_batch))*nb for b,nb in zip(biases,nabla_b)]\n",
    "    #print(weights)\n",
    "    return biases,weights\n",
    "\n",
    "\n",
    "def evaluate(biases,weights,test_data):\n",
    "    test_results=[(np.argmax(feed_forward_eval(biases,weights,x)),y) for x,y in test_data]\n",
    "    return sum(int(x==y) for x,y in test_results)\n",
    "\n",
    "def evaluate_regression(biases,weights,test_data):\n",
    "    #for x,y in test_data:\n",
    "    #    print(feed_forward_eval(biases,weights,x))\n",
    "    #test_results=[np.asscalar(feed_forward_eval(biases,weights,x))-y for x,y in test_data]\n",
    "    #test_results=[(feed_forward_eval(biases,weights,x))-y for x,y in test_data]\n",
    "    test_results=[]\n",
    "    for x,y in test_data:\n",
    "        y=np.reshape(y,(-1,1))\n",
    "        a=(feed_forward_eval(biases,weights,x))\n",
    "        soft= np.exp(a) / sum(np.exp(a))\n",
    "        print(\"soft\")\n",
    "        print(soft)\n",
    "        print(\"cross\")\n",
    "        cross=-(np.sum(y*np.log(soft)))\n",
    "        print(cross)\n",
    "        test_results.append(cross)\n",
    "    print(\"test results\")\n",
    "    print(test_results)\n",
    "    #print(\"soft\")\n",
    "    #soft=np.zeros((layers[-1],1),dtype=float)\n",
    "    #soft= np.exp(test_results[0]) / sum(np.exp(test_results[0]))\n",
    "    #print(soft)\n",
    "    #cross=-(np.sum(y*np.log(predictions+1e-9)))\n",
    "    #test_results=[np.asscalar(feed_forward_eval(biases,weights,x)) for x,y in test_data]\n",
    "    #print(np.dot(test_results,test_results))\n",
    "    #return 0.5*(np.dot(test_results,test_results))\n",
    "    #return 0.5*sum((x-y)*(x-y) for x,y in test_results)\n",
    "    return test_results\n",
    "\n",
    "\n",
    "def sgd(layers,epochs):\n",
    "    random.seed(1000)\n",
    "    np.random.seed(1000)\n",
    "    \n",
    "    biases=[np.random.randn(y,1) for y in layers[1:]]\n",
    "    weights=[np.random.randn(y,x) for x,y in zip(layers[:-1],layers[1:])]\n",
    "    \n",
    "    #train_data1=list(train_data)\n",
    "    #n=len(train_data1)\n",
    "    #test_data1=list(test_data)\n",
    "    #accuracies=[]\n",
    "    regression_costs= []\n",
    "    mini_batch_size = 1\n",
    "    k=1\n",
    "    total_regression_cost=[0]\n",
    "    for j in range(epochs):\n",
    "        a=0  \n",
    "        print('training ...')\n",
    "        a=a+1\n",
    "        train_x=np.zeros((4,1),dtype=float)\n",
    "        train_x=[1,2,3,4]\n",
    "        train_x = [np.resize(train_x,(4,1))]\n",
    "        y_true_batch=[1,0]\n",
    "        train_y = np.reshape(y_true_batch,(1,-1))\n",
    "\n",
    "        training_data = zip(train_x, train_y)\n",
    "\n",
    "        mini_batch = list(training_data)\n",
    "        #print(mini_batch)\n",
    "            \n",
    "        eta = 1.0/a\n",
    "        #update biases and weights\n",
    "        biases,weights=update_mini_batch_sgd(biases,weights,mini_batch,eta)\n",
    "        if(a%10==0):\n",
    "            print('*')\n",
    "            \n",
    "            \n",
    "        a=0\n",
    "        #testing part\n",
    "        print('testing ...')\n",
    "        #total_regression_cost = 0\n",
    "        \n",
    "        a=a+1\n",
    "        #train_y = np.reshape(y_true_batch,(1,-1))\n",
    "\n",
    "        training_data = zip(train_x, train_y)\n",
    "\n",
    "        mini_batch = list(training_data)\n",
    "        #print(mini_batch)\n",
    "\n",
    "        regression_costs = evaluate_regression(biases,weights,mini_batch)\n",
    "        total_regression_cost+=(regression_costs)\n",
    "        print(\"total regression\")\n",
    "        print(total_regression_cost)\n",
    "\n",
    "        #eta = 1.0/a\n",
    "        #update biases and weights\n",
    "        #biases,weights=update_mini_batch_sgd(biases,weights,mini_batch,eta)\n",
    "        #if(a%10==0):\n",
    "        #    print('*')\n",
    "\n",
    "        print('regression cost: %f' %(sum(total_regression_cost)/a))\n",
    "        print('********************')\n",
    "sgd(layers,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
